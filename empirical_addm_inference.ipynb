{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319397cd",
   "metadata": {},
   "source": [
    "# Empirical aDDM Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06fc0f",
   "metadata": {},
   "source": [
    "This notebook will simulate and recover data using `efficient-fpt`'s pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57174051",
   "metadata": {},
   "source": [
    "### Simulating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef12f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "DDM_dir = os.path.abspath('/Users/braydenchien/Desktop/Enkavilab/DDM')\n",
    "sys.path.append(DDM_dir)\n",
    "\n",
    "dt = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b331bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation import get_corrected_empirical_distributions\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv('/Users/braydenchien/Desktop/Enkavilab/DDM/1ms_trial_data.csv')\n",
    "df_raw['choice'] = df_raw['choice'].map({'left':0,'right':1})\n",
    "df_raw['RT'] = df_raw['RT'] / dt # adjustment for RT\n",
    "df_raw['fixation'] = df_raw['fixation'].apply(literal_eval)\n",
    "\n",
    "to_drop = pd.read_csv(\"/Users/braydenchien/Desktop/Enkavilab/DDM/dropped_trials.csv\").rename(columns={\"parcode\": \"sub_id\"})\n",
    "\n",
    "df = df_raw[\n",
    "    ~df_raw.set_index([\"sub_id\", \"trial\"]).index.isin(\n",
    "        to_drop.set_index([\"sub_id\", \"trial\"]).index\n",
    "    )\n",
    "]\n",
    "\n",
    "num_data, _ = df.shape\n",
    "\n",
    "value_diffs = np.unique(df['avgWTP_left'] - df['avgWTP_right'])\n",
    "legend = {\n",
    "    \"left\": {1},\n",
    "    \"right\": {2},\n",
    "    \"transition\": {0}, \n",
    "    \"blank_fixation\": {4}\n",
    "}\n",
    "fixation_col = 'fixation'\n",
    "left_value_col = 'avgWTP_left'\n",
    "right_value_col = 'avgWTP_right'\n",
    "\n",
    "empirical_distributions = get_corrected_empirical_distributions(\n",
    "    df,\n",
    "    value_diffs=value_diffs,\n",
    "    legend=legend,\n",
    "    fixation_col=fixation_col,\n",
    "    left_value_col=left_value_col,\n",
    "    right_value_col=right_value_col,\n",
    "    cutoff=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f753afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "a, b = 1, 0\n",
    "eta = 0.5\n",
    "kappa = 2\n",
    "T = 25 / dt # Bounded by Markov w/p 0.9, E[T] = 2.5, in this case, in ms\n",
    "x0 = 0\n",
    "sigma = 0.7 # As calculated from Eum et al. (2023)\n",
    "\n",
    "# r1_data = np.zeros(num_data)\n",
    "# r2_data = np.zeros(num_data)\n",
    "r1_data = df['avgWTP_left'].to_numpy()\n",
    "r2_data = df['avgWTP_right'].to_numpy()\n",
    "# r1_data = [3]*num_data\n",
    "# r2_data = [3]*num_data\n",
    "\n",
    "seeds = np.random.SeedSequence(123).spawn(num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8cafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available jobs: 8\n",
      "Elapsed time: 214.204 seconds\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed, cpu_count, dump, load\n",
    "from data_utils import simulate_empirical_trial\n",
    "import time\n",
    "\n",
    "print(\"Available jobs:\", cpu_count())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "dump(empirical_distributions, \"empirical.mmap\")\n",
    "empirical_distributions = load(\"empirical.mmap\", mmap_mode=\"r\")\n",
    "\n",
    "results = Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "    delayed(simulate_empirical_trial)(\n",
    "        n,\n",
    "        r1_data,\n",
    "        r2_data,\n",
    "        empirical_distributions,\n",
    "        eta=eta,\n",
    "        kappa=kappa,\n",
    "        sigma=sigma,\n",
    "        a=a,\n",
    "        b=b,\n",
    "        T=T,\n",
    "        x0=x0,\n",
    "        dt=dt,\n",
    "        seed=seeds[n]\n",
    "    )\n",
    "    for n in range(num_data)\n",
    ")\n",
    "\n",
    "print(f\"Elapsed time: {time.time() - start:.3f} seconds\")\n",
    "\n",
    "# Store results\n",
    "decision_data = np.zeros((num_data, 2))\n",
    "flag_data = np.zeros(num_data)\n",
    "mu_data = [None] * num_data\n",
    "sacc_data = [None] * num_data\n",
    "\n",
    "for n, (decision, mu_array, sacc_array, r1, r2, flag) in enumerate(results):\n",
    "    decision_data[n] = decision\n",
    "    mu_data[n] = mu_array\n",
    "    sacc_data[n] = sacc_array\n",
    "    r1_data[n] = r1\n",
    "    r2_data[n] = r2\n",
    "    flag_data[n] = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc6a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data = np.array([len(mu_array) for mu_array in mu_data])\n",
    "max_len = np.max(d_data)\n",
    "\n",
    "mu_data_padded = np.array([np.pad(mu_array, (0, max_len - len(mu_array)), mode='constant') for mu_array in mu_data])\n",
    "sacc_data_padded = np.array([np.pad(sacc_array, (0, max_len - len(sacc_array)), mode='constant') for sacc_array in sacc_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4df0bd",
   "metadata": {},
   "source": [
    "The cell below is a check to see if all the data needed for inference is generated. The source notebook for addm inference unpacks the same data, so this is a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ab2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_save = {\n",
    "#     'mu_array_padded_data': mu_data_padded, # Drift rates for each stage\n",
    "#     'sacc_array_padded_data': sacc_data_padded, # Saccade lengths for each stage\n",
    "#     'd_data': d_data, # Number of stages\n",
    "#     'decision_data': decision_data, # RT and choice\n",
    "#     'r1_data': r1_data, # avgWTP_left\n",
    "#     'r2_data': r2_data, # avgWTP_right\n",
    "#     'flag_data': flag_data.astype(np.int32), # Left choice first\n",
    "#     'eta': eta, # theta (multiplicative discount factor)\n",
    "#     'kappa': kappa, # drift\n",
    "#     'sigma': sigma, # noise\n",
    "#     'a': a, # initial boundary height\n",
    "#     'b': b, # linear boundary collapse rate\n",
    "#     'x0': x0, # initial diffusion distribution\n",
    "#     'T': T, # Maximum simulation horizon\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75763d",
   "metadata": {},
   "source": [
    "### Recovering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1009cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "from scipy.optimize import fmin, minimize, LinearConstraint, Bounds\n",
    "\n",
    "from efficient_fpt.multi_stage_cy import compute_loss_parallel, print_num_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f4a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPE = np.float64\n",
    "\n",
    "# a = data[\"a\"]\n",
    "# b = data[\"b\"]\n",
    "# x0 = data[\"x0\"]\n",
    "# mu1_true = data[\"mu1\"] # Deprecated\n",
    "# mu2_true = data[\"mu2\"] # Deprecated\n",
    "eta_true = eta\n",
    "kappa_true = kappa\n",
    "# r1_data = data[\"r1_data\"]\n",
    "# r2_data = data[\"r2_data\"]\n",
    "flag_data = flag_data.astype(np.int32)\n",
    "\n",
    "# sigma = data[\"sigma\"]\n",
    "# T = data[\"T\"]\n",
    "\n",
    "mu1_true_data = kappa_true * (r1_data - eta_true * r2_data)\n",
    "mu2_true_data = kappa_true * (eta_true * r1_data - r2_data)\n",
    "\n",
    "mu_true_data = mu_data_padded.astype(DATA_TYPE)\n",
    "sacc_data = sacc_data_padded.astype(DATA_TYPE)\n",
    "length_data = d_data.astype(np.int32)\n",
    "rt_data = decision_data[:, 0].astype(DATA_TYPE)\n",
    "choice_data = decision_data[:, 1].astype(np.int32)\n",
    "\n",
    "num_data, max_d = mu_true_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e09842e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available threads: 8\n",
      "# data = 18758\n"
     ]
    }
   ],
   "source": [
    "print_num_threads()\n",
    "print(\"# data =\", num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37b01ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood evaluation time: 0.168 s\n"
     ]
    }
   ],
   "source": [
    "num_iter = 10\n",
    "start_time = time.time()\n",
    "for _ in range(num_iter):\n",
    "    loss = compute_loss_parallel(\n",
    "        mu1_true_data,\n",
    "        mu2_true_data,\n",
    "        rt_data,\n",
    "        choice_data,\n",
    "        flag_data,\n",
    "        sacc_data,\n",
    "        length_data,\n",
    "        max_d,\n",
    "        sigma,\n",
    "        a,\n",
    "        b,\n",
    "        x0,\n",
    "        num_threads=10,\n",
    "    )\n",
    "end_time = time.time()\n",
    "print(f\"Likelihood evaluation time: {(end_time - start_time) / num_iter:.3f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a3f13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numerical optimization for eta, kappa, a, b, x0:\n",
      "Using trust-constr\n",
      "Initial guess: [0.5, 1, 1.0]\n",
      "\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 33, function evaluations: 92, CG iterations: 61, optimality: 4.86e-09, constraint violation: 0.00e+00, execution time: 1.7e+01 s.\n",
      "Total time: 17.342 seconds\n",
      "           message: `gtol` termination condition is satisfied.\n",
      "           success: True\n",
      "            status: 1\n",
      "               fun: 0.6742021889224282\n",
      "                 x: [ 9.991e-01  5.714e-01  7.220e-01]\n",
      "               nit: 33\n",
      "              nfev: 92\n",
      "              njev: 23\n",
      "              nhev: 0\n",
      "          cg_niter: 61\n",
      "      cg_stop_cond: 1\n",
      "              grad: [-1.553e-05  1.490e-08  0.000e+00]\n",
      "   lagrangian_grad: [-2.968e-12 -7.432e-10 -4.860e-09]\n",
      "            constr: [array([ 9.991e-01,  5.714e-01,  7.220e-01])]\n",
      "               jac: [<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "                    \twith 3 stored elements and shape (3, 3)>]\n",
      "       constr_nfev: [0]\n",
      "       constr_njev: [0]\n",
      "       constr_nhev: [0]\n",
      "                 v: [array([ 1.553e-05, -1.564e-08, -4.860e-09])]\n",
      "            method: tr_interior_point\n",
      "        optimality: 4.859875594672846e-09\n",
      "  constr_violation: 0.0\n",
      "    execution_time: 16.636030912399292\n",
      "         tr_radius: 68359375.0\n",
      "    constr_penalty: 1.0\n",
      " barrier_parameter: 1.0240000000000006e-08\n",
      " barrier_tolerance: 1.0240000000000006e-08\n",
      "             niter: 33\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Constraint optimization for searching all parameters\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nNumerical optimization for eta, kappa, a, b, x0:\")\n",
    "method = \"trust-constr\"\n",
    "print(\"Using \" + method)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ORIGINAL 5-PARAMETER OBJECTIVE (COMMENTED OUT)\n",
    "# ------------------------------------------------------------\n",
    "# func = lambda paras: compute_loss_parallel(\n",
    "#     paras[1] * (r1_data - paras[0] * r2_data),\n",
    "#     paras[1] * (paras[0] * r1_data - r2_data),\n",
    "#     rt_data,\n",
    "#     choice_data,\n",
    "#     flag_data,\n",
    "#     sacc_data,\n",
    "#     length_data,\n",
    "#     max_d,\n",
    "#     sigma,\n",
    "#     paras[2],\n",
    "#     paras[3],\n",
    "#     paras[4],\n",
    "# )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UPDATED OBJECTIVE: REDUCED PARAMETER SPACE\n",
    "# ------------------------------------------------------------\n",
    "# We now optimize only [eta, kappa, a].\n",
    "# b and x0 are FIXED to 0 to improve identifiability.\n",
    "#\n",
    "# Rationale:\n",
    "# - Removes collapsing boundary degeneracy (b)\n",
    "# - Removes starting bias compensation (x0)\n",
    "# - Forces asymmetry to be explained via drift\n",
    "# - Improves conditioning of parameter recovery\n",
    "#\n",
    "# Parameter vector:\n",
    "# paras = [eta, kappa, a]\n",
    "\n",
    "func = lambda paras: compute_loss_parallel(\n",
    "    paras[1] * (r1_data - paras[0] * r2_data),\n",
    "    paras[1] * (paras[0] * r1_data - r2_data),\n",
    "    rt_data,\n",
    "    choice_data,\n",
    "    flag_data,\n",
    "    sacc_data,\n",
    "    length_data,\n",
    "    max_d,\n",
    "    sigma,\n",
    "    paras[2],  # a\n",
    "    0.0,       # b fixed to 0 (no collapsing bounds)\n",
    "    0.0,       # x0 fixed to 0 (no starting bias)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# BOUNDS\n",
    "# ------------------------------------------------------------\n",
    "# ORIGINAL 5-PARAMETER BOUNDS (COMMENTED OUT)\n",
    "# [eta, kappa, a, b, x0]\n",
    "#\n",
    "# bounds = Bounds(\n",
    "#     [0, 0, 0, 0, -np.inf],\n",
    "#     [1, np.inf, np.inf, np.inf, np.inf]\n",
    "# )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ORIGINAL \"LOCKED\" VERSION WITH REDUNDANT DIMENSIONS (COMMENTED OUT)\n",
    "# These bounds forced b = 0 and x0 = 0, but still optimized\n",
    "# over 5 dimensions and required unnecessary constraints.\n",
    "#\n",
    "# bounds = Bounds(\n",
    "#     [0, 0, 0, 0, 0],\n",
    "#     [1, np.inf, np.inf, 0, 0]\n",
    "# )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UPDATED 3-PARAMETER BOUNDS\n",
    "# ------------------------------------------------------------\n",
    "# Only optimize over [eta, kappa, a]\n",
    "# All constraints on b and x0 are removed because those\n",
    "# parameters are no longer optimized.\n",
    "#\n",
    "# eta ∈ [0,1]\n",
    "# kappa ≥ 0\n",
    "# a ≥ 0\n",
    "\n",
    "bounds = Bounds(\n",
    "    [0, 0, 0],\n",
    "    [1, np.inf, np.inf],\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LINEAR CONSTRAINTS\n",
    "# ------------------------------------------------------------\n",
    "# ORIGINAL CONSTRAINTS (COMMENTED OUT)\n",
    "#\n",
    "# These enforced:\n",
    "# 1) a - max(rt)*b ≥ 0  (collapsing boundary validity)\n",
    "# 2) |x0| ≤ a          (initial state inside bounds)\n",
    "#\n",
    "# con = LinearConstraint(\n",
    "#     [[0, 0, 1, -np.max(rt_data), 0],\n",
    "#      [0, 0, 1, 0, 1],\n",
    "#      [0, 0, 1, 0, -1]],\n",
    "#     lb=[0, 0, 0],\n",
    "#     ub=[np.inf, np.inf, np.inf],\n",
    "# )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# UPDATED: NO LINEAR CONSTRAINTS NEEDED\n",
    "# ------------------------------------------------------------\n",
    "# Since b = 0 and x0 = 0 are fixed:\n",
    "# - Boundary collapse constraint is irrelevant\n",
    "# - Starting point constraint is automatically satisfied\n",
    "# - a ≥ 0 is already enforced by bounds\n",
    "#\n",
    "# Therefore we REMOVE LinearConstraint entirely.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# INITIAL GUESS\n",
    "# ------------------------------------------------------------\n",
    "# ORIGINAL 5-PARAMETER GUESS (COMMENTED OUT)\n",
    "# initial_guess = [0.5, 1, 1.0, 0.0, 0.0]\n",
    "\n",
    "# Updated 3-parameter guess\n",
    "initial_guess = [0.5, 1, 1.0]\n",
    "\n",
    "print(\"Initial guess:\", initial_guess)\n",
    "print()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OPTIMIZATION\n",
    "# ------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "\n",
    "paras_opt_result = minimize(\n",
    "    func,\n",
    "    x0=initial_guess,\n",
    "    bounds=bounds,\n",
    "    method=method,\n",
    "    options={\"verbose\": 1},\n",
    ")\n",
    "\n",
    "print(f\"Total time: {time.time() - start_time:.3f} seconds\")\n",
    "print(paras_opt_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a6b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True and estimated value of parameters:\n",
      "eta: 0.50000, 0.99912\n",
      "kappa: 2.00000, 0.57142\n",
      "a: 1.00000, 0.72199\n"
     ]
    }
   ],
   "source": [
    "mle = paras_opt_result[\"x\"]\n",
    "print(\"True and estimated value of parameters:\")\n",
    "print(f\"eta: {eta_true:.5f}, {mle[0]:.5f}\")\n",
    "print(f\"kappa: {kappa_true:.5f}, {mle[1]:.5f}\")\n",
    "print(f\"a: {a:.5f}, {mle[2]:.5f}\")\n",
    "# print(f\"b: {b:.5f}, {mle[3]:.5f}\")\n",
    "# print(f\"x0: {x0:.5f}, {mle[4]:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4112b",
   "metadata": {},
   "source": [
    "Without proceeding onto further parameter recovery methods, I believe that having two methods (differential evolution and maximum likelihood estimation) converging on different parameters suggests that the data simulation process does not generate data identifiable to synthetic parameters. Specifically with this model, there is flexibility in collapsing bounds and initial condition, so it does not fit the class of model that I am using. From this, I have three thoughts. First, is it possible to map the transformation of values by carefully selecting different test points? Second, can I constrain the parameter space, like setting x0 to 0 or b to 0? Third, if the crux of the issue is truly the empirical simulation methodology, can some of the mystery be explained by finding out why middle fixations have value-modulated effects if they are sampled iid?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
